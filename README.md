# Hey, I'm Gabe

**Software Engineer @ Microsoft | AI Safety Researcher | Sailor**

I like distributed systems and AI safety. Model Evals are very interesting and I'm open to discussing new opportunities if you want to reach out! Currently fascinated by interpretability and model evaluations.

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/GabeMousa)
[![Email](https://img.shields.io/badge/Email-EA4335?style=flat&logo=gmail&logoColor=white)](mailto:gab.01@hotmail.com)

---

## What I'm Working On

<table>
<tr>
<td width="50%" valign="top">

### Apolien
**AI Safety Evaluation Library for Python**

Building tools to evaluate frontier models for:
- **CoT Faithfulness** — Are models actually reasoning the way they claim?
- **Sycophancy Detection** — Catching specification gaming & reward hacking
- **Deception Testing** — Are the models lying?

Inspired by research from Anthropic's alignment team.

[![Apolien](https://img.shields.io/badge/GitHub-Apolien-a9def9?style=flat&logo=github)](https://github.com/gabe-mousa/Apolien)

</td>
<td width="50%" valign="top">

### @ Microsoft
**Distributed Systems & Infrastructure**

- Authentication/authorization at 5M+ machines/month scale
- MCP-powered infrastructure automation
- Horizontal autoscaling for 1M+ customers/region
- Observability pipelines at billions of requests/month

</td>
</tr>
</table>

---

## Tech Stack

```python
gabriel = {
    "languages": ["Python", "C#/.NET", "Golang", "C++", "Java"],
    "ml_ai": ["TensorFlow", "Scikit-learn", "XGBoost", "Semantic Kernel", "FastMCP"],
    "infrastructure": ["Kubernetes", "Docker", "Azure", "GCP", "AWS"],
    "research_interests": [
        "Mechanistic Interpretability",
        "Pragmatic Interpretability",
        "AI Alignment Research",
        "Chain-of-Thought Faithfulness", 
        "Sycophancy & Reward Hacking"
    ]
}
```

<p>
<img src="https://img.shields.io/badge/Python-ff99c8?style=for-the-badge&logo=python&logoColor=white" />
<img src="https://img.shields.io/badge/C%23-e4c1f9?style=for-the-badge&logo=csharp&logoColor=white" />
<img src="https://img.shields.io/badge/Go-a9def9?style=for-the-badge&logo=go&logoColor=white" />
<img src="https://img.shields.io/badge/TensorFlow-fcf6bd?style=for-the-badge&logo=tensorflow&logoColor=black" />
<img src="https://img.shields.io/badge/Kubernetes-d0f4de?style=for-the-badge&logo=kubernetes&logoColor=black" />
<img src="https://img.shields.io/badge/Azure-a9def9?style=for-the-badge&logo=microsoftazure&logoColor=white" />
</p>

---

## Currently Reading

Papers and research that have my attention:

- [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) — Circuit tracing & attribution graphs in Claude
- [Sycophancy to Subterfuge](https://arxiv.org/abs/2406.10162) — Reward tampering in language models
- [The Case for CoT Unfaithfulness is Overstated](https://www.alignmentforum.org/posts/HQyWGE2BummDCc2Cx/the-case-for-cot-unfaithfulness-is-overstated) — Post-hoc faithfulness analysis

Always looking for paper recommendations in interpretability & alignment!

---

## Let's Connect

I'm always open to:
- Chatting about AI safety, interpretability, or alignment research
- Collaborating on open-source AI safety tooling
- Discussing new opportunties if you want to reach out

**Reach out:** [LinkedIn](https://www.linkedin.com/in/GabeMousa) or [gab.01@hotmail.com](mailto:gab.01@hotmail.com)

---

<p align="center">
<i>AI Safety is cool, I'd love to talk if you want to reach out</i>
</p>
